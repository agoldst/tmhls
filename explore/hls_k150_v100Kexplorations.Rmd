Explorations
------------

```{r setup}
setwd("~/Documents/research/20c/hls/tmhls")
library(Matrix)
source("analyze_model.R")
m <- do.call(analyze_model,model_files("hls_k150_v100K"))
# tym_result:
load("models/hls_k150_v100K/tym.rda")
m$dtw <- merge(m$doctops,m$metadata[,c("id","pubdate")],by="id")
m$yrly <- tm_yearly_totals(tm_wide=m$dtw)
m$dtm <- doc_topics_matrix(m$doctops)
corpus_meta <- m$metadata[m$metadata$id %in% m$id_map,]
m$n <- length(unique(m$wkf$topic))
# raw counts conditional on journal and year
m$yrly_j <- tm_yearly_totals_meta(m$doctops,m$metadata,vars="journaltitle")
```

```{r tdm,eval=F}
# don't execute unless needed...
# tdm:
load("models/hls_k150_v100K/tdm.rda")
```

That's `r nrow(corpus_meta)` documents in the modeled corpus. How many authors?

```{r}
authors <- unlist(strsplit(corpus_meta$author,"\t",fixed=T))
length(authors)
length(unique(authors))
```

# The literature topic
```{r}
tm_yearly_line_plot(topics=94,raw_counts=T,.yearly_totals=m$yrly)
```

```{r}
mallet_word_plot(c("literature","language"),tym_result$tym,tym_result$yseq,m$vocab,plot_total=F)
```

```{r}
words <- topic_top_words(94,m$wkf,n=50)
mallet_word_plot(words,tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

But be careful about "new literary history"...

```{r}
words <- setdiff(words,c("new","literary","history"))
mallet_word_plot(words,tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

```{r}
tm_yearly_journals_plot(94,m$doctops,m$metadata,m$yrly)
```

And dropping the journal *NLH* from the topic count:

```{r}
yrly_without <- function(without) {
    without_frame <- ddply(subset(m$yrly_j,!(journaltitle %in% without)),
                    .(pubdate),function (d) {
                        colSums(subset(d,select=-c(pubdate,journaltitle)))
                    })
    result <- t(as.matrix(subset(without_frame,select=-pubdate)))
    colnames(result) <- without_frame$pubdate
    rownames(result) <- rownames(m$yrly) # just steal "topic1","topic2"...
    stopifnot(all(colnames(result)==colnames(m$yrly)))
    result
}


tm_yearly_line_plot(topics=94,raw_counts=T,
                    .yearly_totals=yrly_without("New Literary History\t"),
                    .yearly_overall=colSums(m$yrly))
```

# The criticism topic

```{r}
mallet_word_plot(topic_top_words(16,m$wkf,n=50),tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

```{r}
tm_yearly_line_plot(topics=16,raw_counts=F,.yearly_totals=m$yrly)
```

Which topics hold the word in top 50?

```{r}
with(m,wkf$topic[wkf$word=="criticism"])
with(m,wkf$topic[wkf$word=="critic"])
```

```{r}
mallet_word_plot(c("criticism","critic"),tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

Which articles?

```{r}
# utility function
cite_top_documents <- function(topic,n=10) { 
  docs_frame <- top_documents(topic,m$id_map,m$dtm,n)
  data.frame(citation=cite_articles(m$metadata,docs_frame$id),
             weight=docs_frame$weight)
}
```

# The all-in-one

```{r}
# omit topics parameter to get them all
p <- tm_yearly_line_plot(.yearly_totals=m$yrly,facet=T)
# better to have a full rectangle
p + facet_wrap(~topic,nrow=10)
```

# facts and evidence?

```{r}
mallet_word_plot(c("fact","facts","evidence","interpretation"),tym_result$tym,tym_result$yseq,m$vocab)
```

# verify doctops against state

```{r eval=F}
# build verification file
system("python python/doc_topics.py models/hls_k150_v100K/state_simple.csv 150 > models/hls_k150_v100K/doc_topics_check.csv")
check_doctops <- read.csv("models/hls_k150_v100K/doc_topics_check.csv",as.is=T,header=F)
all(check_doctops==m$dtm)
```

# probing tf-idf scores

```{r eval=F}
tdm_tfidf <- tdm_tm(tdm)
tdm_tfidf <- TermDocument_sparse(tdm_tfidf)
search_term <- "found"
hits <- m$id_map[order(tdm_tfidf[match(search_term,m$vocab),],decreasing=T)[1:5]]
cite_articles(m$metadata,hits)
```


# Misclassified items that got into the corpus

```{r}
modphil <- subset(corpus_meta,journaltitle=="Modern Philology\t")
# reviews are in 
Epp <- subset(modphil,grepl("E",pagerange))
```

There are `r nrow(Epp)` misclassified reviews from *Modern Philology*:

```{r results="asis"}
cite_articles(Epp)
```

# Splitting out yearly counts by journal

Let's take a look at topic `r topic_name(16,m$wkf)`.


```{r}
top16_journals_p <- topic_yearly_journals_plot(16,.yrly_j=m$yrly_j)
print(top16_journals_p)
```


What happens if we take out *CI*? *PMLA*?

```{r}
top16_journals_p %+% subset(m$yrly_j,journaltitle != "Critical Inquiry\t")
```

```{r}
top16_journals_p %+% subset(m$yrly_j,!(journaltitle %in%
                                      c("Critical Inquiry\t","PMLA\t")))
```

# The grid of all topics over the years

Here's the overall grid, with journal breakdowns as well:
```{r grid-journal-breakdown}
yj_m <- melt(m$yrly_j)
tj_p <- ggplot(yj_m,
               aes(x=as.Date(pubdate),
                   y=value,
                   group=journaltitle,
                   fill=journaltitle))
tj_p <- tj_p + geom_area() + facet_wrap(~ variable,nrow=10)
# ggsave("models/hls_k150_v100K/report/topic_journal_grid.png",w=12,h=8)
```


146 looks like an oddball: `r topic_name(146,m$wkf,n=30)`. But it may be more coherent that we think. Even removing those initial very common words doesn't change its trend shape. Compare:

```{r}
tm_yearly_line_plot(.yearly_totals=m$yrly,topics=146)
```

and:

```{r}
w146 <- topic_top_words(146,m$wkf,n=30)
print(w146)
```

```{r}
mallet_word_plot(w146,tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

and:

```{r}
mallet_word_plot(w146[7:30],tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

I would guess the high-frequency words that are the topic's top words are actually not the anchors of the topics but have just been stuck in by the algorithm in order to reinforce the trend. As for what this topic is about, let's go ahead and ("bloom") call it the conservative fucks topic:

```{r}
cite_articles(m$metadata,top_documents(146,m$id_map,m$dtm,n=15)$id)
```

However, there are a few topics that are very prominent on the grid and also not easy to interpret:

```{r}
for(i in c(14,52,59,114)) {
  print(topic_name(i,m$wkf,n=10))
}
```

The overall grid is thus a little clearer if we zoom in on the y axis, losing the tops of the biggest topics:

```{r}
tj_p + ylim(0,0.2)
```



# exclude a journal from 1-gram picture

```{r}
tym_ci <- term_year_matrix_journal("Critical Inquiry\t",m$metadata,tdm,m$id_map,m$vocab)
# check yseq's match: they should
stopifnot(all(tym_result$yseq==tym_ci$yseq))
tym_no_ci <- tym_result$tym - tym_ci$tym
mallet_word_plot(c("criticism","critic","critics","critical"),tym_no_ci,tym_result$yseq,m$vocab,plot_total=T)
```

```{r}
mallet_word_plot(c("criticism","critic","critics","critical"),tym_result$tym,tym_result$yseq,m$vocab,plot_total=T)
```

# break up topics into individual words over time

```{r}
topic <- 16
filename <- sprintf("models/hls_k150_v100K/tytm/%03d.rda",topic)
#tytm_result
load(filename)
words <- topic_top_words(topic,m$wkf,n=8)
mallet_word_plot(words,tytm_result$tym,tytm_result$yseq,m$vocab,
                 plot_freq=F,plot_total=F) + facet_wrap( ~ word)
```


# compare a word's overall frequency with its assignment to topics

This overlays lines showing the time series of the occurrences of a word in the corpus with the number of occurrences of the word allocated to each of the topics.

```{r}
word_topic_breakdown <- function(words,geom="area",verbose=T,plot_freq=F,
                                 topics=1:m$n) { 
  # single word, but could tally up total for multiple words
  show_total <- length(words) > 1
  series_list <- vector("list",m$n)
  series_keep <- logical(m$n)
  cur <- 1
  topic_labels <- character()
  if(plot_freq) {
    totals <- colSums(tym_result$tym)   # totals for *all* topics
  } else {
    totals <- NULL
  }
  for(topic in topics) {
    load(sprintf("models/hls_k150_v100K/tytm/%03d.rda",topic))
    series <- term_year_series_frame(words,
                                     tytm_result$tym,tytm_result$yseq,
                                     m$vocab,
                                     raw_counts=!plot_freq,
                                     total=show_total,
                                     denominator=totals)
    if(any(series$weight > 0)) {
      if(verbose) {
        message("Hits in series for topic",topic)
      }
      # series$topic <- sprintf("%03d",topic)
      series$topic <- topic_name(topic,m$wkf,n=2)
      series_list[[cur]] <- series
      cur <- cur + 1
    } # (if)
  } # (for)
  series_frame <- do.call(rbind,series_list)
  series_plot <- mallet_word_plot(words,
                                     tym_result$tym,tym_result$yseq,
                                     m$vocab,
                                     plot_freq=plot_freq,
                                     plot_total=show_total)
  if(geom=="line") {
    result <- series_plot + geom_line(data=series_frame,
                                      aes(group=topic,color=topic))
  } else if(geom=="area") {
    result <- series_plot + geom_area(data=series_frame,
                                      aes(group=topic,fill=topic))
  } else if(geom=="area_fill") {
    result <- ggplot(series_frame,aes(x=year,y=weight,
                                      group=topic,fill=topic)) +
      geom_area(position="fill")
  } else if(geom=="none") {
    result <- ggplot(series_frame)
  } else {
    stop("I don't know what to do with the geom parameter ",geom)
  }

  result <- result + ggtitle(paste("Occurrences of",
                                   paste(words,collapse="/"),
                                   "in topics"))
  result
}
```

Thinking about topic 16, take a look at:
```{r}
word_topic_breakdown("criticism")
```

And for 94:
```{r}
word_topic_breakdown("literary")
```

```{r}
word_topic_breakdown("reading")
```

And for 20:
```{r}
word_topic_breakdown("interpretation")
```

If you look at the frequency over the whole corpus:

```{r}
mallet_word_plot("interpretation",tym_result$tym,tym_result$yseq,m$vocab,plot_freq=F)
```

you see a scary-looking spike in the 1890s. This is not as scary as might seem, since the frequency in the 1890s is relative to many fewer total words and articles. Still, it turns out that there is one article with an extraordinarily large count of "interpretation"s:

```{r eval=F}
# Get the whole term document matrix to go hunting in
load("models/hls_k150_v100K/tdm.rda")
w <- match("interpretation",m$vocab) # 3051
c19ids <- m$metadata$id[pubdate_Date(m$metadata$pubdate) < as.Date("1900-01-01")]
# In fact there is only one hit in c19 in the top 100:
stopifnot(sum(m$id_map[order(tdm[w,],decreasing=T)[1:100]] %in% c19ids)==1)
hit <- which(m$id_map[order(tdm[3051,],decreasing=T)[1:100]] %in% c19ids)
hit_id <- (m$id_map[order(tdm[w,],decreasing=T)[1:100]])[hit]
# the citation
cite_articles(m$metadata,hit_id)
# "J. Douglas Bruce, \"The Anglo-Saxon Version of the Book of Psalms Commonly 
# Known as the Paris Psalter,\" *PMLA* 9, no. 1 (January 1894): 43-164."
# the word count
tdm[w,match(hit_id,m$id_map)] # 101
```


# Foreign

hand coded listing of foreign-language topics

````
003 OE gone after 1910
*005 French names and titles, rise to 1920, fading glory after 1960
011 Spanish some docs to 1930
024 German names and title words peaks 1900s, 1940s
034 Italian some through 1920s, one outlier ca. 1952 (10.2307/460030)
**045 Spanish more presence in 1930-1970
070 vdn daz man [OHG / MHG] up through 1910
*076 French: rising to 1950, falling down, but always present
078 plus montaigne rabelais decline to 1950
081 latin quae esse a few docs through 1940
083 oe mgh ohg [OE etymology] through the 1930s
092 pe pat hym [ME with thorns], a few docs in early years only
*095 German presence through 1960
*103 italian titles: sporadic, appears on both ends of the century
*107 French titles: steadily present  after 1910 in small amounts (< 0.5%)
120 spanish portuguese cid: a few docs in 1910s, 1940
**124 French rise to 1950, declinng through 1980
127 text ms judas also includes Latin, present only before 1930
132 Old french: pre 1930 only
136 poe dutch german [misc. Germanic and Scandinavian], only in a few 1890 and 1930 outliers
139 german germany mann: German titles and authors: steady rise from 1950
141 same rilke mir: German: marginal except for docs in 1940, 1950

*140 english language words

047 [chaucer and middle english]
061 [piers plowman pearl up through 1920]
065 [elizabethan english]
````

My wrong way of counting negative postwar trends:
```{r}
foreign_topics <- c(3,5,11,24,34,45,70,76,78,81,83,92,95,103,107,120,124,127,132,136,139,141)
nyrs <- ncol(m$yrly)
y0 <- which(colnames(m$yrly) == "1940-01-01")
for(i in foreign_topics) { print(cor.test(y0:nyrs,m$yrly[i,y0:nyrs],alternative="less")) }
```

Wrong because it's a bunch of confidence tests, because these are time series with auto-dependencies, because the topics themselves are not independent, etc.

Anyway, I hand-count 15 "significant" negative correlations

# N.B.

128: hyphenations

# "power" for Ted

```{r}
word_topic_breakdown("power",geom="area_fill")
word_topic_breakdown("power",geom="area",plot_freq=T)
word_topic_breakdown("power",geom="area",plot_freq=F)
word_topic_breakdown("power",geom="line",plot_freq=T)
word_topic_breakdown("power",geom="line")
mallet_word_plot("power")
```


# topics with proper names in (by hand)

looking at `keys_summary.csv`, i.e. top 50 words

004 hobbes
005 voltaire, rousseau, etc
006 shakespeare
008 derrida, heidegger
011 lope, quevedo, etc
012 marston collier
020 reading text reader ... woolf ... miller
021 aristotle plato etc
022 jonson marlowe middleton etc
024 goethe schiller lessing
026 black white ... faulkner ... baldwin ... bois
028 hawthorne ... bloom ... finnegans
032 donne chapman marvell
034 piu tasso
035 coleridge hazlitt
037 knight romance ... malory ... chretien
038 nature natural man world .... [near bottom] lucretius
039 interpretation meaning text ... fish
040 ship voyage captain island crusoe robinson marlow ... defoe 
042 figure sterne picture
043 game cervantes play
045 spanish mas spain ... unamuno garcia
046 pastoral arcadia ... drayton ...  (nb 'sidney' on stoplist)
047 chaucer tale troilus
049 american new america whitman ... twain ...
051 french vv roman guillaume ... bertran ... troubadour
054 political english england burke government
055 bacon new knowledge society ... browne ...
058 social work form own ideology ... marxism (nb 'marx' on stoplist)
060 religious christian ... jesus ... bunyan
061 piers plowman pearl ... langland
063 faust faustus stein play ... marlowe ... wagner
064 pope dryden english essay
076 baudelaire suis moi mort
077 human moral own world ... wittgenstein ... rorty
078 plus montaigne rabelais french ronsard
084 esthetic romantic ... nietzsche idea pater
085 new rev university victorian ... browning
088 film cinema films see image ... kane ... barthes
089 greek epic classical ... ovid ... vergil (nb 'homer' on stoplist)
091 eliot yeats pound new ruskin waste moore auden land lewes
094 literary literature new work ...... frye
096 poetry poet poetic poems poets poem ... dickinson ... frost ... crane
097 paradise god lost satan ... miltonic ... (nb 'milton' on stoplist)
099 animal animals ....... aesop
101 skelton bishop gypsies
102 feeling emotional moral pleasure .... smith
103 italian italy boccaccio florence ... petrarch ...
105 swift sublime essay gulliver ... locke ...
106 wordsworth keats nature poet
107 french paris france balzac proust
108 violence trial crime memory ... shoah arendt 
109 sexual feminist sex sexuality desire ... sappho ... sedgwick ... foucault
110 philosophy science philosophical knowledge ... ortega
111 dickens novel victorian ... thackeray
113 sonnet sonnets petrarch form day
115 tragedy play drama [.....aristotle way down]
120 spanish portuguese...menendez
122 chinese china japanese .... wang ... weil
125 jewish jews hebrew ... said ... [wonder if Said gets a boost from "said"]
129 russian soviet russia tolstoy bakhtin
130 body medical hardy health [hardy is intrusive here among the "health" words]
133 ms manuscript [....waaay down at bottom:] pepys
134 spenser faerie book
135 desire self freud
136 poe dutch german ... ibsen ...
137 stevens new wilde figure [actually a henry james topic, but henry and james are stopped out!]
139 german germany mann kafka
141 same rilke mir war mich
146 own like toward ... [down around 20:] bloom
147 fielding jones defoe
148 irish scottish ... gray ... scots sir burns
150 novel novels fiction ... miss austen


...

[131 play plays macbeth caesar ...]
004 a genuinely surprising law topic, driven by NLH/CI

# other novelties

019 see new media
036 economic money value
069 world european national colonial cultural
070 vnd daz man nit mir hat wol wolfram ... gottfried


```{r}
mallet_word_plot(c("men","women"),tym_result$tym,tym_result$yseq,m$vocab)
```


## note on 020 reading text reader ... woolf

Hmm. Woolf has been wholly absorbed into 020 (because of "The Common *Reader*"?)

```{r}
topic_word_breakdown("woolf",plot_freq=T)
```

We can look more closely at the documents where Woolf is prominent to see if "reading" etc. are prominent too...

```{r}
tf_idf <- function(term,doc,tdm) {
    idf <- log(ncol(tdm) / rowSums(tdm[term,,drop=F] != 0))
    Diagonal(n=length(term),x=idf) %*% tdm[term,doc]
}
```

term weightings for topic 20 top words in documents in which "woolf" occurs at all:
```{r}
# remember to load the tdm
w20 <- m$wkf$word[m$wkf$topic==20][1:15]
w20 <- w20[w20 != "woolf"]
t_w20 <- match(w20,m$vocab)
t_woolf <- match("woolf",m$vocab)
tf_idf(c(t_woolf,t_w20),which(tdm[t_woolf,] != 0),tdm)
```

Or we could look at some association measures:
```{r}
x0 <- tdm[match("reader",m$vocab),]
for(w in c("woolf","reading","text")) {
    term <- match(w,m$vocab)
    x1 <- tdm[term,]
    print(w)
    print(cor(x0,x1))
    print((x0 %*% x1) / (sqrt(x0 %*% x0) * sqrt(x1 %*% x1)))
}
```

```{r}
words <- topic_top_words(20,m$wkf,n=8)
mallet_word_plot(words,tym_result$tym,tym_result$yseq,m$vocab,plot_total=F) +
    scale_color_brewer(palette="Set1")
```

Conclusion: "woolf" is positively associated with the top words in topic 20, but not at all closely


# jameson

```{r}
jameson <- m$metadata$id[grepl("jameson",m$metadata$author,ignore.case=T)]
# remove non-fredrics
jameson <- jameson[jameson %in% with(m$metadata,id[grepl("fredric",author,ignore.case=T)])]
# this also drops 
# Wilhelm Dilthey and Frederic Jameson, \"The Rise of Hermeneutics,\" *New Literary History*
# 3, no. 2 (January 1972): 229-244.
sum(sapply(jameson,function(j) { 58 %in% order(m$doctops[m$doctops$id==j,1:150],decreasing=T)[1:3] }))
```

# topic 067 see inquiry particular form

this is a fairly coherent "abstract debate over method" topic, which, like other theory topics, peaks ca. 1980 and then drops. Unfortunately "inquiry" is v. prominent in part because this is a *CI*-heavy topic:

```{r}
tm_yearly_journals_plot(67,m$doctops,m$metadata,.yrly_j=m$yrly_j)
```

Still:

```{r}
tm_yearly_line_plot(topics=67,raw_counts=T,
                    .yearly_totals=yrly_without("Critical Inquiry\t"),
                    .yearly_overall=colSums(m$yrly))
```

picks out a similar peak.

# recents

Hand-coding recent risers:

```{r depends="grid-journal-breakdown"}
recents <- c(015,143,138,058,019,025,048,069,036,004,108,077,102)             
yj_m_recent <- subset(yj_m,variable %in% paste("topic",recents,sep=""))
ggplot(yj_m_recent,aes(x=as.Date(pubdate),y=value,
                       group=journaltitle,
                       fill=journaltitle)) +
   geom_area() + facet_wrap(~ variable,nrow=4)
```

# alternate labeling strategies

## Blei and Lafferty's scoring

```{r blei_wkf,eval=F}
m$b <- read.csv("models/hls_k150_v100K/params.csv")$beta
m$tw <- read_topic_words_matrix("models/hls_k150_v100K/topic_words.csv")
m$scores <- topic_word_scores(m$tw,m$b)
m$a <- m$wkf$alpha[match(1:m$n,m$wkf$topic)]
m$wkf_blei <- topic_words_wkf(m$scores,m$vocab,m$a,n_top=nrow(m$wkf) / m$n)
```

## correlations

```{r}
# just a wrapper for cor on corresponding rows of 2 matrices
word_topic_cor <- function (n,m1,m2) {
    suppressWarnings(cor(m1[n,],m2[n,]))
}

# make sure tym is loaded first.
top_corr_words <- function(topic,n) {
    load(sprintf("models/hls_k150_v100K/tytm/%03d.rda",topic))
    words <- match(m$wkf$word[m$wkf$topic==topic],m$vocab)
    tym_m <- as.matrix(tym_result$tym)
    tytm_m <- as.matrix(tytm_result$tym)
    wtcs <- sapply(words,word_topic_cor,
                   m1=tym_m,m2=tytm_m)
    o <- order(wtcs,decreasing=T)[1:n]
    data.frame(topic=topic,word=m$vocab[words[o]],r=wtcs[o])
}

make_label_frame <- function(n_words=10) {
  lst <- vector("list",m$n)
  for(i in seq(m$n)) {
      message("Working on topic ",i)
      lst[[i]] <- top_corr_words(i,n_words)
  }
  do.call(rbind,lst)
}
```

```{r}
n_label_words <- 10
lfrm <- make_label_frame(n_label_words)
write.csv(lfrm,"models/hls_k150_v100K/tytm_corr_labels.csv",quote=F)
labelings <- sapply(seq(m$n) - 1,function(i) {
                    paste(lfrm$word[i * n_label_words + seq(n_label_words)],
                          collapse=" ")
                   })
labelings <- paste(sprintf("%03d",seq(m$n)),labelings)
writeLines(labelings,"models/hls_k150_v100K/tytm_corr_labels.txt")
```
